{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrmgUUTu70xf",
        "toc": "true"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Factorization-Machine-(FM)\" data-toc-modified-id=\"Factorization-Machine-(FM)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Factorization Machine (FM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Implementation</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ebXsOU70xj"
      },
      "source": [
        "# Factorization Machines lab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6bYDfx70xj"
      },
      "source": [
        "**Factorization Machines** (FMs) are prediction/classification algorithms that combine traditional linear regression with advanced matrix factorization.\n",
        "\n",
        "The *cool* idea behind FMs is to model interactions between features (aka. attributes, explanatory variables) using factorized parameters.\n",
        "By doing so, FMs have the ability to estimate all interactions between features even with extremely sparse data.\n",
        "\n",
        "This presentation is largely taken from [an earlier work](http://ethen8181.github.io/machine-learning/recsys/factorization_machine/factorization_machine.html) by [Ethen](https://github.com/ethen8181/) who is gratefully acknowledged.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFcs4YbA70xj"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6cM2Acj70xk"
      },
      "source": [
        "Normally, when we think of linear regression, we would think of the following formula:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i}\n",
        "\\end{align}\n",
        "\n",
        "Where:\n",
        "\n",
        "- $w_0$ is the bias term, a.k.a intercept.\n",
        "- $w_i$ are weights corresponding to each feature vector $x_i$, here we assume we have $n$ total features.\n",
        "\n",
        "This formula's advantage is that it can computed in linear time, $O(n)$. The drawback, however, is that it does not handle feature interactions. To capture interactions, we could introduce a weight for each feature combination. This is sometimes referred to as a $2_{nd}$ ordered polynomial. The resulting model is shown below:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
        "\\end{align}\n",
        "\n",
        "Compared to our previous model, this formulation has the advantages that it can capture feature interactions at least for two features at a time. But we have now ended up with a $O(n^2)$ complexity which means that to train the model, we now require a lot more time and memory. Another issue is that when we have categorical variables with high cardinality, after one-hot encoding them, we would end up with a lot of columns that are sparse, making it harder to actually capture their interactions (not enough data).\n",
        "\n",
        "To solve this complexity issue, Factorization Machines takes inspiration from [matrix factorization](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/recsys/1_ALSWR.ipynb), and models the feature interaction using latent factors. Every feature $f_i$ has a corresponding latent factor $v_i$, and two features' interactions are modelled as $\\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle$, where $\\langle \\cdot \\;,\\cdot \\rangle$ refers to the dot product of the two feature vector. If we assume its of size $k$ (this is a hyperparameter that we can tune). Then:\n",
        "\n",
        "\\begin{align}\n",
        "\\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle = \\sum_{f=1}^k v_{i,f} v_{j,f}\n",
        "\\end{align}\n",
        "\n",
        "This leads of our new equation:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\sum_{i=1}^{n} \\sum_{j=i+1}^n \\langle \\textbf{v}_i , \\textbf{v}_{j} \\rangle x_i x_{j}\n",
        "\\end{align}\n",
        "\n",
        "This is an improvement from our previous model (when we modeled each pair of interaction terms with weight $w_{ij}$) as the number of parameters is reduced from $n^2$ to $n \\times k$, since $k \\ll n$, which also helps mitigate overfitting issues. Using the naive way of formulating factorization machine results in a complexity of $O(kn^2)$, because all pairwise interactions have to be computed, but we can reformulate it to make it run in $O(kn)$.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
        "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j} - \\frac{1}{2} \\sum_{i=1}^n \\langle \\textbf{v}_i , \\textbf{v}_{i} \\rangle x_{i} x_{i}  \\\\\n",
        "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j} \\right) - \\frac{1}{2}\\left( \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
        "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j}  -  \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
        "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{j=1}^n v_{j,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right) \\\\\n",
        "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
        "\\end{align}\n",
        "\n",
        "Note, summing over different pairs is the same as summing over all pairs minus the self-interactions (divided by two). This is the reason why the value 1/2 is introduced from the beginning of the derivation.\n",
        "\n",
        "This reformulated equation has a linear complexity in both $k$ and $n$, i.e. its computation is in $O(kn)$, substituting this new equation into the existing factorization machine formula, we end up with:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhEdPJyq70xk"
      },
      "source": [
        "In a machine learning setting, factorization machine can be applied to different supervised prediction tasks:\n",
        "\n",
        "- **Regression:**, in this case $\\hat{y}(\\textbf{x})$ can be used directly by minimizing the mean squared error between the model prediction and target value, e.g. $\\frac{1}{N}\\sum^{N}\\big(y - \\hat{y}(\\textbf{x})\\big)^2$\n",
        "- **Classification:**, if we were to use it in a binary classification setting, we could then minimize the log loss, $\\ln \\big(e^{-y \\cdot \\hat{y}(\\textbf{x})} + 1 \\big)$, where $\\sigma$ is the sigmoid/logistic function and $y \\in {-1, 1}$.\n",
        "\n",
        "To train factorization machine, we can use a gradient descent based optimization techniques, the parameters to be learned are $(w_0, \\mathbf{w},$ and $\\mathbf{V}$).\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial\\theta}\\hat{y}(\\textbf{x}) =\n",
        "\\begin{cases}\n",
        "1,  & \\text{if $\\theta$ is $w_0$} \\\\\n",
        "x_i, & \\text{if $\\theta$ is $w_i$} \\\\\n",
        "x_i\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i}^2 & \\text{if $\\theta$ is $v_{i,f}$}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "- Notice that $\\sum_{j=1}^n v_{j, f} x_j$ does not depend on $i$, thus it can be computed independently.\n",
        "- The last formula above, can also be written as $x_i(\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i})$.\n",
        "- In practice, we would throw in some L2 regularization to prevent overfitting.\n",
        "\n",
        "As the next section contains implementation of the algorithm from scratch, the gradient of the log loss is also provided here for completeness. The predicted value $\\hat{y}(\\textbf{x})$ is replaced with $x$ for making the notation cleaner.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{d}{dx}\\left[ \\ln \\big(e^{-yx} + 1 \\big) \\right]\n",
        "&= \\frac{1}{e^{-yx} + 1} \\cdot  \\frac{d}{dx}\\left[e^{-yx} + 1 \\right] \\\\\n",
        "&= \\frac{\\frac{d}{dx}\\left[e^{-yx} \\right] + \\frac{d}{dx}\\left[1 \\right]}{e^{-yx} + 1} \\\\\n",
        "&= \\frac{e^{-yx} \\cdot \\frac{d}{dx}\\left[-yx \\right] + 0}{e^{-yx} + 1} \\\\\n",
        "&= \\frac{e^{-yx} \\cdot -y}{e^{-yx} + 1} \\\\\n",
        "&= -\\frac{ye^{-yx}}{e^{-yx} + 1} \\\\\n",
        "&= -\\frac{y}{e^{yx} + 1}\n",
        "\\end{align}\n",
        "\n",
        "---\n",
        "\n",
        "**Advantages:** We'll now wrap up the theoretical section of factorization machine, with some of its advantages:\n",
        "\n",
        "- We can observe from the model equation that it can be computed in linear time.\n",
        "- By leveraging ideas from matrix factorization, we can estimate higher order interaction effects even under very sparse data.\n",
        "- Compared to traditional matrix factorization methods, which is restricted to modeling a user-item matrix, we can leverage other user or item specific features making factorization machine more flexible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import njit\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These Scikit-learn submodules are needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The running example is the well-known Wisconsin Breast Cancer dataset, which is embedded in Scikit-learn. \n",
        "\n",
        "It's a binary classification problem; dimensions are extracted from scans.\n",
        "\n",
        "Can FMs capture quadratic interactions between those features? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyC1MZYu70xl"
      },
      "outputs": [],
      "source": [
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "labelencoder_Y = LabelEncoder()\n",
        "\n",
        "y = labelencoder_Y.fit_transform(y)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "print(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A direct implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the implementation of factorization machine Ethen uses a `for` loop based code for readability. \n",
        "\n",
        "Such implementation can be based on [Cython or Numba](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/python/cython/cython.ipynb); this solution uses Numba.\n",
        "\n",
        "Run the cell below to create the class, no need to go over the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZPpIh7wA70xm"
      },
      "outputs": [],
      "source": [
        "class FactorizationMachineClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Factorization Machine [1]_ using Stochastic Gradient Descent.\n",
        "    For binary classification only.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_iter : int, default 10\n",
        "        Number of iterations to train the algorithm.\n",
        "\n",
        "    n_factors : int, default 10\n",
        "        Number/dimension of features' latent factors.\n",
        "\n",
        "    learning_rate : float, default 0.1\n",
        "        Learning rate for the gradient descent optimizer.\n",
        "\n",
        "    reg_coef : float, default 0.01\n",
        "        Regularization strength for weights/coefficients.\n",
        "\n",
        "    reg_factors : float, default 0.01\n",
        "        Regularization strength for features' latent factors.\n",
        "\n",
        "    random_state : int, default 1234\n",
        "        Seed for the randomly initialized features latent factors\n",
        "\n",
        "    verbose : bool, default True\n",
        "        Whether to print progress bar while training.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    intercept_ : double\n",
        "        Intercept term, w0 based on the original notations.\n",
        "\n",
        "    coef_ : 1d ndarray, shape [n_features,]\n",
        "        Coefficients, w based on the original notations.\n",
        "\n",
        "    feature_factors_ : 2d ndarray, shape [n_factors, n_features]\n",
        "        Latent factors for all features. v based on the original\n",
        "        notations. The learned factors can be viewed as the\n",
        "        embeddings for each features. If a pair of features tends\n",
        "        to co-occur often, then their embeddings should be\n",
        "        close/similar (in terms of cosine similarity) to each other.\n",
        "\n",
        "    history_ : list\n",
        "        Loss function's history at each iteration, useful\n",
        "        for evaluating whether the algorithm converged or not.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] `S. Rendle Factorization Machines (2010)\n",
        "            <http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf>`_\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter = 10, n_factors = 10,\n",
        "                 learning_rate = 0.1, reg_coef = 0.01,\n",
        "                 reg_factors = 0.01, random_state = 1234, verbose = False):\n",
        "        self.n_iter = n_iter\n",
        "        self.verbose = verbose\n",
        "        self.reg_coef = reg_coef\n",
        "        self.n_factors = n_factors\n",
        "        self.reg_factors = reg_factors\n",
        "        self.random_state = random_state\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the input data and label.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
        "            Data in sparse matrix format.\n",
        "\n",
        "        y : 1d ndarray, shape [n_samples,]\n",
        "            Training data's corresponding label.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        self.coef_ = np.zeros(n_features)\n",
        "\n",
        "        self.intercept_ = np.array([0.0])\n",
        "\n",
        "        # the factors are often initialized with a mean of 0 and standard deviation\n",
        "        # of 1 / sqrt(number of latent factor specified)\n",
        "        np.random.seed(self.random_state)\n",
        "\n",
        "        self.feature_factors_ = np.random.normal(\n",
        "            scale = 1 / np.sqrt(self.n_factors), size = (self.n_factors, n_features))\n",
        "\n",
        "        # the gradient is implemented in a way that requires\n",
        "        # the negative class to be labeled as -1 instead of 0\n",
        "        y = y.copy().astype(np.int32)\n",
        "\n",
        "        y[y == 0] = -1\n",
        "\n",
        "        loop = range(self.n_iter)\n",
        "\n",
        "        if self.verbose:\n",
        "            loop = trange(self.n_iter)\n",
        "\n",
        "        self.history_ = []\n",
        "        #print(X)\n",
        "        #print(X.indptr)\n",
        "        #print(X.indices)\n",
        "        for _ in loop:\n",
        "\n",
        "            loss = _sgd_update(X.data, X.indptr, X.indices,\n",
        "                               y, n_samples, n_features,\n",
        "                               self.intercept_, self.coef_,\n",
        "                               self.feature_factors_, self.n_factors,\n",
        "                               self.learning_rate, self.reg_coef, self.reg_factors)\n",
        "            #print(loss)\n",
        "\n",
        "            self.history_.append(loss)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Probability estimates. The returned estimates for\n",
        "        all classes are ordered by the label of classes.\n",
        "\n",
        "        Paramters\n",
        "        ---------\n",
        "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
        "            Data in sparse matrix format.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        proba : 2d ndarray, shape [n_samples, n_classes]\n",
        "            The probability of the sample for each class in the model.\n",
        "        \"\"\"\n",
        "        pred = self._predict(X)\n",
        "\n",
        "        pred_proba = 1.0 / (1.0 + np.exp(-pred))\n",
        "\n",
        "        proba = np.vstack((1 - pred_proba, pred_proba)).T\n",
        "\n",
        "        return proba\n",
        "\n",
        "    def _predict(self, X):\n",
        "        \"\"\"Similar to _predict_instance but vectorized for all samples\"\"\"\n",
        "        linear_output = X * self.coef_\n",
        "        v = self.feature_factors_.T\n",
        "\n",
        "        term = (X.dot(v)) ** 2 - (X.power(2).dot(v ** 2))\n",
        "\n",
        "\n",
        "        factor_output = 0.5 * np.sum(term, axis = 1)\n",
        "\n",
        "        return self.intercept_ + linear_output + factor_output\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
        "            Data in sparse matrix format.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Predicted class label per sample.\n",
        "        \"\"\"\n",
        "        pred_proba = self.predict_proba(X)[:, 1]\n",
        "\n",
        "        return pred_proba.round().astype(int)\n",
        "\n",
        "\n",
        "@njit\n",
        "def _sgd_update(data, indptr, indices, y, n_samples, n_features,\n",
        "                w0, w, v, n_factors, learning_rate, reg_w, reg_v):\n",
        "    \"\"\"\n",
        "    Compute the loss of the current iteration and update\n",
        "    gradients accordingly.\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        pred, summed = _predict_instance(data, indptr, indices, w0, w, v, n_factors, i)\n",
        "\n",
        "        # calculate loss and its gradient\n",
        "        loss += _log_loss(pred, y[i])\n",
        "\n",
        "        loss_gradient = -y[i] / (np.exp(y[i] * pred) + 1.0)\n",
        "\n",
        "        # update bias/intercept term\n",
        "        w0[0] -= learning_rate * loss_gradient\n",
        "\n",
        "        # update weight\n",
        "        for index in range(indptr[i], indptr[i + 1]):\n",
        "\n",
        "            feature = indices[index]\n",
        "\n",
        "            w[feature] -= learning_rate * (loss_gradient * data[index] + 2 * reg_w * w[feature])\n",
        "\n",
        "        # update factor\n",
        "        for factor in range(n_factors):\n",
        "            for index in range(indptr[i], indptr[i + 1]):\n",
        "\n",
        "                feature = indices[index]\n",
        "\n",
        "                term = summed[factor] - v[factor, feature] * data[index]\n",
        "                v_gradient = loss_gradient * data[index] * term\n",
        "                v[factor, feature] -= learning_rate * (v_gradient + 2 * reg_v * v[factor, feature])\n",
        "\n",
        "    loss /= n_samples\n",
        "    return loss\n",
        "\n",
        "\n",
        "@njit\n",
        "def _predict_instance(data, indptr, indices, w0, w, v, n_factors, i):\n",
        "    \"\"\"predicting a single instance\"\"\"\n",
        "    summed = np.zeros(n_factors)\n",
        "    summed_squared = np.zeros(n_factors)\n",
        "\n",
        "    # linear output w * x\n",
        "    pred = w0[0]\n",
        "    for index in range(indptr[i], indptr[i + 1]):\n",
        "        feature = indices[index]\n",
        "        pred += w[feature] * data[index]\n",
        "\n",
        "    # factor output\n",
        "    for factor in range(n_factors):\n",
        "        for index in range(indptr[i], indptr[i + 1]):\n",
        "            feature = indices[index]\n",
        "            term = v[factor, feature] * data[index]\n",
        "            summed[factor] += term\n",
        "            summed_squared[factor] += term * term\n",
        "\n",
        "        pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
        "\n",
        "    # summed is the independent term that can be re-used\n",
        "    # during the gradient update stage\n",
        "    return pred, summed\n",
        "\n",
        "\n",
        "@njit\n",
        "def _log_loss(pred, y):\n",
        "    \"\"\"\n",
        "    negative log likelihood of the\n",
        "    current prediction and label, y.\n",
        "    \"\"\"\n",
        "    return np.log(np.exp(-pred * y) + 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training and application "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z_cLPkcnUMx"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "4EIa-qceH8kd",
        "outputId": "2f6e374f-a6f9-4dad-97a1-60cf750a3951"
      },
      "outputs": [],
      "source": [
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "X_train_csr = csr_matrix(X_train_scaled)\n",
        "\n",
        "# Train FM\n",
        "fm = FactorizationMachineClassifier(n_iter=100, \n",
        "                                    learning_rate=0.0001, \n",
        "                                    reg_coef=0.01, \n",
        "                                    reg_factors=0.01, \n",
        "                                    verbose=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#fm.fit(X_train_csr, Y_train_fixed)\n",
        "fm.fit(X_train_csr, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "OVmU-0L370xn",
        "outputId": "58923580-7786-4291-86cc-05415a04e182"
      },
      "outputs": [],
      "source": [
        "# change default style figure and font size\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 8, 6\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# one quick way to check that we've implemented\n",
        "# the gradient descent is to ensure that the loss\n",
        "# curve is steadily decreasing\n",
        "plt.plot(fm.history_)\n",
        "plt.title('Loss Curve Per Iteration')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we apply our FM to the test set and calculate the `AUC` score (area under the curve)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FMMo5iO70xn",
        "outputId": "9296da8f-2fbb-4219-a8cd-7429fa131ec2"
      },
      "outputs": [],
      "source": [
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "X_test_csr = csr_matrix(X_test_scaled)\n",
        "\n",
        "y_pred_prob = fm.predict_proba(X_test_csr)\n",
        "\n",
        " #print(y_pred_prob)\n",
        "fm_auc = roc_auc_score(Y_test, y_pred_prob[:, 1])\n",
        "\n",
        "print('Factorization Machine AUS =', fm_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparative assessment\n",
        "\n",
        "We compare the FM performance on the test set with a baseline Linear regression model *trained on the same data.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-DNIzbo70xn",
        "outputId": "7c08041b-3850-41fc-c68e-624f22df8571"
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "logreg.fit(X_train, Y_train)\n",
        "\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "lr_auc = roc_auc_score(Y_test, y_pred_prob)\n",
        "\n",
        "print('Linear Regression AUC =', lr_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'The difference in AUC is', round((fm_auc - lr_auc)*100, 2), '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECGdvarv70xn"
      },
      "source": [
        "There are various open-sourced implementations floating around the web, here are the links to some of them:\n",
        "\n",
        "- https://github.com/ibayer/fastFM\n",
        "- https://github.com/srendle/libfm\n",
        "- https://github.com/aksnzhy/xlearn\n",
        "- https://github.com/scikit-learn-contrib/polylearn\n",
        "\n",
        "I personally haven't tested which one is more efficient, feel free to grab one of them as see if it helps solve your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AW5HS2s_70xn"
      },
      "source": [
        "# Reference\n",
        "\n",
        "- [Blog: Factorization Machines](http://www.jefkine.com/recsys/2017/03/27/factorization-machines/)\n",
        "- [Blog: Deep Understanding of FFM Principles and Practices (Chinese)](https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html)\n",
        "- [Quora: What are the drawbacks of Factorization Machines?](https://www.quora.com/What-are-the-drawbacks-of-Factorization-Machines)\n",
        "- [Paper: S. Rendle Factorization Machines (2010)](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
