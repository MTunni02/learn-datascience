---
title: "Data Science as 9 problems"
lang: en
author: AP 
format:
  revealjs: 
    theme: solarized
    css: ../../styles/dsta_slides.css
    slide-number: true
    slide-level: 2
    # title-slide-attributes:
      # data-background-image: ../../styles/bbk-logo.svg
    code-fold: false
    echo: true
    # smaller: true
    scrollable: true
    # auto-stretch: false
  html:
    toc: true
    code-fold: false
    anchor-sections: true
    other-links:
      - text: Class page
        href: https://ale66.github.io/learn-datascience/
jupyter: python3
---


## A gentle-yet-focussed introduction

[Chapter 2](./provost-ch2.pdf) describes the core computational problems of Data Science

![](./imgs/provost-cover.jpg){.nostretch fig-align="center" width="200px"}


In memoriam: [Tom Fawcett](https://link.springer.com/article/10.1007/s10994-020-05909-0)

-----

1.  Classification and class probability

__Instance:__

* a collection (dataset) of datapoints from $\mathbf{X}$

* a classification system $C = \{c_1, c_2, \dots c_k\}$

. . .

__Solution:__  classification function $\gamma: \mathbf{X} \rightarrow C$

__Measure:__ misclassification


-----

## Misclassification: detection 

binary, only one class is important

![](./imgs/false_positives_and_negatives.jpg){width="40%"}

-----

## Misclassification: multi-class 

![](./imgs/misclass-examples.png)


-----

![](./imgs/misclassification.png)


[Source](https://www.researchgate.net/publication/322596493_A_PCA-CCA_network_for_RGB-D_object_recognition)


-----

2. Regression/value estimation

__Instance:__

* a collection (dataset) of numerical $<\mathbf{x}, y>$ datapoints

* a regressor (independent) value $\mathbf{x}$

. . .

__Solution:__  a regressand (dependent) value $y$

that complements $\mathbf{x}$

__Measure:__ error over the collection

. . .

[PF] "classification predicts whether something will happen, whereas regr. predicts how much something will happen."


-----

3. Similarity

Identify similar individuals based on data known about them.

__Instance:__

* a collection (dataset) of datapoints from $\mathbf{X}$, e.g., $\mathbb{R}^n$

* (distance functions for some of the dimensions)

. . .

__Solution:__  similarity function $\sigma: \mathbf{X} \rightarrow \mathbb{R}$

[__Measure:__ error]


-----

Good similarity measures are the key to accurate detection/classification


![](./imgs/similarity.png)

<!-- http://dingcvnote.blogspot.com/2018/06/matlab-comparing-of-detect-feature.html -->


-----

4. Clustering (segmentation)

group individuals in a population together by their similarity (but not driven by any specific purpose)

__Instance:__

* a collection (dataset) $\mathbf{D}$ of datapoints from $\mathbf{X}$, e.g., $\mathbb{R}^n$

* a relational structure on $\mathbf{X}$ (a graph)

* a small integer $k$

. . .

__Solution:__  a partition of $\mathbf{D}$ into $\mathcal{C_1}, \dots \mathcal{C_k}$

__Measure:__ network modularity Q: proportion of the relational structure that _respects_ the clusters.

-----

Detection version: $k$ is part of the output.

See an [example research work  (from yours truly)](
https://www.sciencedirect.com/science/article/pii/S0022000013000767)

-----

5. Co-occurence (frequent itemset mining)

similarity of objects based on their appearing together in transactions.

__Instance:__

* a collection (dataset) $\mathbf{T}$ of itemsets (subsets of  $\mathbf{X}$) or sequences

* a theshold $\tau$

. . .

__Solution:__  All _frequent patterns:_ subsets that appear in $\mathbf{T}$ above $\tau$

. . .

Detection version: $\tau$ is part of the output.

Market-basket analysis, (some) recommendation systems

-----

6. Profiling (behaviour description)

__Instance:__

* a user description $\mathbf{u}$ drawn from a $\mathbf{D}$ collection

* a stimulus $a\in \mathbf{A}$

* a set of possible responses $\mathbf{R}$

. . .

__Solution:__  a functional reaction of __u__ to __a__, i.e., $\rho: \mathbf{U} \times \mathbf{A} \rightarrow \mathbf{R}$

. . .

Application: anomaly/fraud detection.

Example research work on [Social media profiling](https://ieeexplore.ieee.org/abstract/document/6994286)

-----

7. Link prediction

__Instance:__  a dynamical [graph (network) $\mathbf{G}$](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) , i.e., a sequence

$<V, E>$,

$<V, E^\prime=E+\{(u,v)\}>$,

$<V, E^{\prime\prime}=E^\prime+\{(r,s)\}>\dots$

![](./imgs/network.png)

-----

__Question:__  what is the next link to be created?

What YouTube video will you watch next?

Alternatives: predict the __strength__ of the new link; link deletion.

-----

8. Data reduction

__Instance:__

* a collection (dataset) $\mathbf{D}$ of datapoints from $\mathbf{X}$, e.g., $\mathbb{R}^m$

* [a distinct independent variable $x_i$]

. . .

__Solution:__  a projection of $\mathbf{D}$ onto $\mathbb{R}^n$, $n < m$

__Measure:__ error in the estimation of $x_i$

Example: genre identification in consumer behaviour analysis

-----

9. Causal modelling

__Instance:__

* a collection (dataset) $\mathbf{D}$ of datapoints from $\mathbf{X}$, e.g., $\mathbb{R}^m$

* a distinct dependent variable $x_i$

. . .

__Solution:__  a variable $x_j$ of $\mathbf{D}$ that controls $x_i$

__Measure:__  effectiveness of $x_j$ *tuning* to *tune* $x_i$ in turn.

. . .

Example: Exactly What food causes you to put on weight?

Controlled clinical trials, A/B testing.


<!-- ------------------------- -->
# From problems to algorithms

-----

Computer Science is often metaphoric: it uses abstractions that allow us to 

- focus our problem-solving

- communicate with the computer

Examples:

- interpreted languages (Python)
  
- data models (SQL and relational tables)

- TCP/IP computer networks


-----

Problem $\longrightarrow$ Algorithm $\longrightarrow$ Implementation (code)

For a given problem, more than one algorithm may be available

For a given algorithm, more than one implementation is possible

Only with clarity about the problem we can look for the algorithms.

-----

## Supervised Algorithms

Previous instance/solution pairs are available and fed to the a.

A. may 'memorise' past solutions and re-apply them, via some similarity criterion

A. may also 'learn' a model and apply it to future inputs 


-----

## Method 

* obtain a dataset of examples, inc. the  "target"  dimension, called *label*

* split it in training and test data

* run a. on the test data, find a putative solution

* test the quality/pred. power against test data

. . .

Regression has  a numeric target while classification has a categorical/binary one

-----

## P. with good supervised algorithms

1: Regression

2: Classification

9: Causal Modelling


-----

## P. with mostly unsupervised a.

4: Clustering

5: co-occurrence grouping

6: profiling


-----

## P. with a mix of supervised and unsupervised

3: Similarity matching,

7: link prediction,

8: data reduction